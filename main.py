import requests
from bs4 import BeautifulSoup
import pandas as pd
import re


def get_article_ids_from_hidden_links(page_number=1):
    url = f"https://www.daai.tv/news/history?p={page_number}"
    print(f"ğŸŒ æ­£åœ¨æŠ“å–ç¬¬ {page_number} é ï¼š{url}")
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")

    hidden_div = soup.select_one('div[style="display:none"]')
    if not hidden_div:
        print("âš ï¸ æ²’æ‰¾åˆ°éš±è—é€£çµå€å¡Š")
        return []

    article_ids = []
    for a in hidden_div.find_all("a", href=True):
        match = re.search(r"/news/history/(\d{6})", a["href"])
        if match:
            article_ids.append(match.group(1))

    print(f"âœ… å…±æŠ“åˆ° {len(article_ids)} å‰‡æ–°è ID")
    return article_ids


def extract_news_from_article(article_url):
    response = requests.get(article_url)
    soup = BeautifulSoup(response.text, 'html.parser')
    meta_tag = soup.find("meta", attrs={"name": "description"})
    if not meta_tag or not meta_tag.get("content"):
        print(f"âš ï¸ æ‰¾ä¸åˆ° meta descriptionï¼š{article_url}")
        return None

    raw_text = meta_tag["content"]
    sections = re.split(r"\n?â—", raw_text)
    sections = [s.strip() for s in sections if s.strip()]

    column_texts = []
    label = None
    for section in sections:
        match = re.match(r"(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥\s+(.+?)\n", section)
        if match:
            year, month, day, title = match.groups()
            label = f"{int(month)}æœˆ{int(day)}è™Ÿ"
            full_title = f"â—{year}å¹´ {title}"
            content = section[match.end():].strip()
            content = re.sub(r"\s+", " ", content)
            combined = f"{full_title}\n{content}"
            column_texts.append(combined)
    if not column_texts:
        return None
    return label, column_texts


def run_scraper(page_number=1):
    article_ids = get_article_ids_from_hidden_links(page_number)

    all_rows = []
    for article_id in article_ids:
        url = f"https://www.daai.tv/news/history/{article_id}"
        print(f"ğŸ“° æ“·å–æ–‡ç« ï¼š{url}")
        result = extract_news_from_article(url)
        if result:
            label, column_texts = result
            row = [label] + column_texts
            all_rows.append(row)

    if all_rows:
        max_len = max(len(r) for r in all_rows)
        for row in all_rows:
            while len(row) < max_len:
                row.append("")

        columns = ["æ—¥æœŸ"] + [f"æ–°è{i+1}" for i in range(max_len - 1)]
        df = pd.DataFrame(all_rows, columns=columns)

        output_path = f"å¤§æ„›æ–°è_ç¬¬{page_number}é 24å‰‡æ–°è.xlsx"
        df.to_excel(output_path, index=False)
        print(f"âœ… åŒ¯å‡ºå®Œæˆï¼š{output_path}")
    else:
        print("âŒ æ²’æœ‰æˆåŠŸæ“·å–ä»»ä½•è³‡æ–™")


# === æŠ“å¤šé  ===
for page_number in range(5, 7):  
    print(f"\nğŸ“„ æ­£åœ¨è™•ç†ç¬¬ {page_number} é ")
    run_scraper(page_number)
